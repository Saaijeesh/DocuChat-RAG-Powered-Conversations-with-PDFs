{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saaijeeshsn/Desktop/Langchain/.venv/lib/python3.9/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n",
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#web based loader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import bs4\n",
    "\n",
    "#load, chunk and index the content of the html page\n",
    "\n",
    "loader=WebBaseLoader(web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "                     bs_kwargs=dict(parse_only=bs4.SoupStrainer(\n",
    "                         class_=(\"post-title\",\"post_content\",\"post-header\")\n",
    "                     )))\n",
    "\n",
    "text_documents=loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter=RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "documents=text_splitter.split_documents(text_documents)\n",
    "documents[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the context of artificial intelligence and autonomous agents, task decomposition refers to the process of breaking down a complex problem or task into smaller, manageable subtasks. This approach allows for more efficient and effective solution strategies by enabling individual components to be solved independently and then combined to achieve the overall goal.\n",
      "\n",
      "Task decomposition is an essential technique in many areas of AI research, including planning, scheduling, control, and multi-agent systems. It plays a crucial role in enabling intelligent agents to learn, reason, and act effectively in complex environments by allowing them to tackle problems incrementally and adapt their strategies based on the feedback from the environment or other agents.\n",
      "\n",
      "Here's an example of task decomposition in a simple scenario: suppose we have a robot tasked with assembling a piece of furniture. The high-level goal is \"assemble the furniture.\" However, this goal can be decomposed into several subgoals such as:\n",
      "1. Locate all necessary components (e.g., screws, bolts, panels).\n",
      "2. Pick up each component using the appropriate tool (e.g., hand, wrench).\n",
      "3. Move each component to the appropriate location on the furniture.\n",
      "4. Follow the instructions in the assembly manual to assemble the furniture piece by piece.\n",
      "5. Tighten any screws or bolts when necessary.\n",
      "6. Verify that all components have been correctly assembled.\n",
      "\n",
      "Each of these subgoals is a smaller, more manageable task that can be tackled independently and then combined to achieve the overall goal of assembling the furniture. This process of breaking down complex problems into smaller, manageable subtasks is called task decomposition.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"mistral\")\n",
    "vectorstore = Chroma.from_documents(documents=documents,embedding=embeddings)\n",
    "\n",
    "#create retriever\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in text_documents)\n",
    "\n",
    "#define ollama llm function\n",
    "def ollama_llm(question,context):\n",
    "    formatted_prompt = f\"Question: {question}\\n\\nContext: {context}\"\n",
    "    response = ollama.chat(model='mistral',messages=[{'role':'user','content':formatted_prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "def rag_chain(question):\n",
    "    retrieved_docs = retriever.invoke(question)\n",
    "    formatted_context = format_docs(retrieved_docs)\n",
    "    return ollama_llm(question, formatted_context)\n",
    "\n",
    "result=rag_chain(\"What is Task decomposition?\")\n",
    "print(result)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
